{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zroL1Om3keT9"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import scipy.stats as stats\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    precision_recall_curve,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    auc\n",
        ")\n",
        "import yfinance as yf\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)"
      ],
      "metadata": {
        "id": "WuZUj3kwk68M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for time series data\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "u-xzz4Uy-pAo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FinancialDataHandler:\n",
        "    def __init__(self,\n",
        "                 ticker_info={'EURUSD':'EURUSD=X'},\n",
        "                 start_date='2020-01-01',\n",
        "                 end_date=None,\n",
        "                 window_size=30,\n",
        "                 train_split=0.8,\n",
        "                 model_save_path='.'):\n",
        "        \"\"\"\n",
        "        Initialize the model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        ticker : dict\n",
        "            Dictionary containing ticker name and ticker symbol\n",
        "        start_date : str\n",
        "            Start date for data collection (YYYY-MM-DD)\n",
        "        end_date : str\n",
        "            End date for data collection (YYYY-MM-DD), defaults to current date\n",
        "        window_size : int\n",
        "            Size of the input window (how many past days to use)\n",
        "        train_split : float\n",
        "            Proportion of data to use for training, defaults to 0.8\n",
        "        model_save_path : str\n",
        "            Path to save the model, defaults to current directory\n",
        "\n",
        "        \"\"\"\n",
        "        self.ticker_name = list(ticker_info.keys())[0]\n",
        "        self.ticker = list(ticker_info.values())[0]\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')\n",
        "        self.window_size = window_size\n",
        "        self.train_split = train_split\n",
        "        self.model_save_path = model_save_path\n",
        "\n",
        "        # data containers\n",
        "        self.raw_data = None\n",
        "        self.data = None\n",
        "\n",
        "    def fetch_data(self):\n",
        "        \"\"\"\n",
        "        Fetch data from Yahoo Finance and save the raw data to a CSV file\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            Raw data\n",
        "        \"\"\"\n",
        "        print(f\"Fetching {self.ticker_name} data from {self.start_date} to {self.end_date}\")\n",
        "\n",
        "        try:\n",
        "            # Yahoo Finance ticker for EUR/USD\n",
        "            self.raw_data = yf.download(\n",
        "                self.ticker,\n",
        "                start=self.start_date,\n",
        "                end=self.end_date,\n",
        "                interval=\"1d\",\n",
        "                progress=False\n",
        "            )\n",
        "\n",
        "            if self.raw_data.empty:\n",
        "                print(f\"No data found for {self.ticker}\")\n",
        "                return None\n",
        "\n",
        "            # Save raw data to CSV\n",
        "            csv_path = os.path.join(f'{self.model_save_path}/data', f\"{self.ticker}_raw.csv\")\n",
        "            self.raw_data.to_csv(csv_path)\n",
        "            print(f\"Raw data saved to {csv_path}\")\n",
        "\n",
        "            return self.raw_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {self.ticker} data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_and_preprocess_data(self):\n",
        "        \"\"\"\n",
        "        Fetch data and calculate technical indicators, then preprocess the data\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            Preprocessed data\n",
        "        \"\"\"\n",
        "\n",
        "        # Fetch data\n",
        "        data = self.fetch_data()\n",
        "\n",
        "        # Calculate technical indicators\n",
        "        df = self._calculate_technical_indicators(data)\n",
        "\n",
        "        # Preprocess data\n",
        "        processed_data = self._preprocess_data(df)\n",
        "\n",
        "        self.data = processed_data\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def _calculate_technical_indicators(self, df):\n",
        "        \"\"\"\n",
        "        Calculate technical indicators\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame containing forex data\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            DataFrame with added technical indicators\n",
        "        \"\"\"\n",
        "        print(\"Calculating technical indicators\")\n",
        "\n",
        "        # Make a copy to avoid modifying the original dataframe\n",
        "        df = df.copy()\n",
        "\n",
        "        # Moving averages\n",
        "        df['MA5'] = df['Close'].rolling(window=5).mean()\n",
        "        df['MA10'] = df['Close'].rolling(window=10).mean()\n",
        "        df['MA20'] = df['Close'].rolling(window=20).mean()\n",
        "\n",
        "        # Bollinger Bands (20-day, 2 standard deviations)\n",
        "        df['BB_middle'] = df['Close'].rolling(window=20).mean()\n",
        "        df['BB_std'] = df['Close'].rolling(window=20).std()\n",
        "        df['BB_upper'] = df['BB_middle'] + 2 * df['BB_std']\n",
        "        df['BB_lower'] = df['BB_middle'] - 2 * df['BB_std']\n",
        "\n",
        "        # RSI (Relative Strength Index) - 14 days\n",
        "        delta = df['Close'].diff()\n",
        "        gain = delta.where(delta > 0, 0)\n",
        "        loss = -delta.where(delta < 0, 0)\n",
        "        avg_gain = gain.rolling(window=14).mean()\n",
        "        avg_loss = loss.rolling(window=14).mean()\n",
        "        rs = avg_gain / avg_loss\n",
        "        df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "        # MACD (Moving Average Convergence Divergence)\n",
        "        df['EMA12'] = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "        df['EMA26'] = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "        df['MACD'] = df['EMA12'] - df['EMA26']\n",
        "        df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
        "        df['MACD_hist'] = df['MACD'] - df['MACD_signal']\n",
        "\n",
        "        # Price changes\n",
        "        df['Price_Change'] = df['Close'].pct_change()\n",
        "        df['Price_Change_5d'] = df['Close'].pct_change(periods=5)\n",
        "\n",
        "        # Volatility indicators\n",
        "        df['HL_Diff'] = df['High'] - df['Low']\n",
        "        df['HL_Diff_Pct'] = (df['High'] - df['Low']) / df['Low']\n",
        "\n",
        "        # ATR (Average True Range) - 14 days\n",
        "        high_low = df['High'] - df['Low']\n",
        "        high_close = (df['High'] - df['Close'].shift()).abs()\n",
        "        low_close = (df['Low'] - df['Close'].shift()).abs()\n",
        "        ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
        "        true_range = ranges.max(axis=1)\n",
        "        df['ATR'] = true_range.rolling(14).mean()\n",
        "\n",
        "        # Drop NaN values\n",
        "        df = df.dropna()\n",
        "\n",
        "        print(\"Technical indicators calculated\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _preprocess_data(self, df):\n",
        "        \"\"\"\n",
        "        Preprocess data for LSTM model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame containing forex data with technical indicators\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Dictionary containing preprocessed data\n",
        "        \"\"\"\n",
        "        print(\"Preprocessing data\")\n",
        "\n",
        "        # Select features\n",
        "        features = [\n",
        "            'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "            'MA5', 'MA10', 'MA20',\n",
        "            'BB_upper', 'BB_middle', 'BB_lower',\n",
        "            'RSI', 'MACD', 'MACD_signal', 'MACD_hist',\n",
        "            'Price_Change', 'Price_Change_5d',\n",
        "            'HL_Diff', 'HL_Diff_Pct', 'ATR'\n",
        "        ]\n",
        "\n",
        "        # Scale the features\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        scaled_data = scaler.fit_transform(df[features])\n",
        "        scaled_df = pd.DataFrame(scaled_data, index=df.index, columns=features)\n",
        "\n",
        "        # Create sequences for LSTM\n",
        "        X, y = [], []\n",
        "        for i in range(len(scaled_df) - self.window_size):\n",
        "            X.append(scaled_df.iloc[i:i+self.window_size].values)\n",
        "\n",
        "            # target: price direction (up/down) after the window_size 1: positive, 0: negative direction\n",
        "            pct_idx = features.index('Price_Change')\n",
        "            y.append(1 if df.iloc[i+self.window_size, pct_idx] > 0 else 0)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        # Split into train and test sets\n",
        "        split_idx = int(len(X) * self.train_split)\n",
        "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "        # Save dates for visualization\n",
        "        dates = df.index[self.window_size:].tolist()\n",
        "        train_dates = dates[:split_idx]\n",
        "        test_dates = dates[split_idx:]\n",
        "\n",
        "        # Save preprocessed data\n",
        "        np.save(os.path.join(f'{self.model_save_path}/data', \"X_train.npy\"), X_train)\n",
        "        np.save(os.path.join(f'{self.model_save_path}/data', \"y_train.npy\"), y_train)\n",
        "        np.save(os.path.join(f'{self.model_save_path}/data', \"X_test.npy\"), X_test)\n",
        "        np.save(os.path.join(f'{self.model_save_path}/data', \"y_test.npy\"), y_test)\n",
        "\n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            'features': features,\n",
        "            'close_idx': pct_idx,\n",
        "            'train_dates': [d.strftime('%Y-%m-%d') for d in train_dates],\n",
        "            'test_dates': [d.strftime('%Y-%m-%d') for d in test_dates]\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(f'{self.model_save_path}/data', \"metadata.pth\"), 'wb') as f:\n",
        "            pickle.dump(metadata, f)\n",
        "\n",
        "        with open(os.path.join(f'{self.model_save_path}/data', \"scaler.pth\"), 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "\n",
        "        print(f\"Preprocessed data saved. X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "        return {\n",
        "            'X_train': X_train,\n",
        "            'y_train': y_train,\n",
        "            'X_test': X_test,\n",
        "            'y_test': y_test,\n",
        "            'metadata': metadata,\n",
        "            'scaler': scaler,\n",
        "            'raw_df': df\n",
        "        }"
      ],
      "metadata": {
        "id": "p4JwZPlGwYMV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention mechanism for LSTM\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attention = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        # lstm_output shape: (batch_size, seq_len, hidden_size)\n",
        "        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)\n",
        "        # attention_weights shape: (batch_size, seq_len, 1)\n",
        "\n",
        "        context_vector = torch.sum(attention_weights * lstm_output, dim=1)\n",
        "        # context_vector shape: (batch_size, hidden_size)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class StackedLSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Stacked LSTM model with attention mechanism\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size_1=128, hidden_size_2=64, hidden_size_3=32,\n",
        "                 dropout_rate=0.2, output_size=1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size_1 = hidden_size_1\n",
        "        self.hidden_size_2 = hidden_size_2\n",
        "        self.hidden_size_3 = hidden_size_3\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # First LSTM layer\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_size_1, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Second LSTM layer\n",
        "        self.lstm2 = nn.LSTM(hidden_size_1, hidden_size_2, batch_first=True)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = AttentionLayer(hidden_size_2)\n",
        "\n",
        "        # Third LSTM layer\n",
        "        self.lstm3 = nn.LSTM(hidden_size_2 * 2, hidden_size_3, batch_first=True)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_size_3, output_size)\n",
        "\n",
        "    def forward(self, x, apply_dropout=False):\n",
        "        # x shape: (batch_size, seq_len, input_size)\n",
        "\n",
        "        # First LSTM layer\n",
        "        lstm1_out, _ = self.lstm1(x)\n",
        "        if apply_dropout:\n",
        "            lstm1_out = self.dropout1(lstm1_out)\n",
        "        else:\n",
        "            lstm1_out = lstm1_out * (1 - self.dropout_rate)\n",
        "\n",
        "        # Second LSTM layer\n",
        "        lstm2_out, _ = self.lstm2(lstm1_out)\n",
        "        if apply_dropout:\n",
        "            lstm2_out = self.dropout2(lstm2_out)\n",
        "        else:\n",
        "            lstm2_out = lstm2_out * (1 - self.dropout_rate)\n",
        "\n",
        "        # Attention mechanism\n",
        "        context_vector, attention_weights = self.attention(lstm2_out)\n",
        "        context_vector_expanded = context_vector.unsqueeze(1).expand(-1, lstm2_out.size(1), -1)\n",
        "        combined = torch.cat((lstm2_out, context_vector_expanded), dim=2)\n",
        "\n",
        "        # Third LSTM layer\n",
        "        lstm3_out, _ = self.lstm3(combined)\n",
        "        if apply_dropout:\n",
        "            lstm3_out = self.dropout3(lstm3_out)\n",
        "        else:\n",
        "            lstm3_out = lstm3_out * (1 - self.dropout_rate)\n",
        "\n",
        "        # ooutput at last time step\n",
        "        last_time_step = lstm3_out[:, -1, :]\n",
        "\n",
        "        # Output layer\n",
        "        output = (self.fc(last_time_step))\n",
        "        output = nn.Sigmoid()(output)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "5a_9owzy6aig"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictionModel:\n",
        "\n",
        "    def __init__(self, ticker, data, start_date, end_date, device, save_model_path= '.',optimized_params=None):\n",
        "        \"\"\"\n",
        "        Initialize the model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        ticker : dict\n",
        "            Dictionary containing ticker name and ticker symbol (yfinance)\n",
        "        device : torch.device\n",
        "            Device to run the model on\n",
        "        data : dict\n",
        "            Dictionary containing preprocessed data\n",
        "        date : tuple\n",
        "            Tuple containing start and end dates\n",
        "        save_model_path : str\n",
        "            Path to save the model, defaults to current directory\n",
        "        optimized_params : dict\n",
        "            Dictionary containing model parameters\n",
        "        \"\"\"\n",
        "\n",
        "        self.ticker_name = list(ticker.keys())[0]\n",
        "        self.ticker = list(ticker.values())[0]\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')\n",
        "        self.data = data\n",
        "        self.device = device\n",
        "        self.model_save_path = save_model_path\n",
        "\n",
        "        # Initialize data containers\n",
        "        self.model = None\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = None\n",
        "        self.metrics = None\n",
        "\n",
        "        # Default hyperparameters\n",
        "        self.params = {\n",
        "            'dropout_rate': 0.2,\n",
        "            'learning_rate': 0.0005,\n",
        "            'batch_size': 32,\n",
        "            'hidden_size_1': 128,\n",
        "            'hidden_size_2': 64,\n",
        "            'hidden_size_3': 32\n",
        "        }\n",
        "\n",
        "        # Update params with opitmized params if provided\n",
        "        if optimized_params:\n",
        "          self.params.update(optimized_params)\n",
        "\n",
        "        print(f\"Initialized Model with parameters: {self.params}\")\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build the stacked LSTM model with optimized hyperparameters\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        model : StackedLSTMModel\n",
        "            Built PyTorch model\n",
        "        \"\"\"\n",
        "        if self.data is None:\n",
        "            print(\"Data not available. Please run fetch_and_preprocess_data first.\")\n",
        "            return None\n",
        "\n",
        "        input_size = self.data['X_train'].shape[2]\n",
        "        print(f\"Building stacked LSTM model with input size {input_size}\")\n",
        "\n",
        "        # Create model\n",
        "        model = StackedLSTMModel(\n",
        "            input_size=input_size,\n",
        "            hidden_size_1=self.params['hidden_size_1'],\n",
        "            hidden_size_2=self.params['hidden_size_2'],\n",
        "            hidden_size_3=self.params['hidden_size_3'],\n",
        "            dropout_rate=self.params['dropout_rate']\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Print model summary\n",
        "        print(model)\n",
        "        print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_model(self, epochs=100, patience=20):\n",
        "        \"\"\"\n",
        "        Train the model with optimized hyperparameters\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        epochs : int\n",
        "            Maximum number of epochs\n",
        "        patience : int\n",
        "            Patience for early stopping\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        model : StackedLSTMModel\n",
        "            Trained PyTorch model\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"Model not built. Please run build_model first.\")\n",
        "            return None\n",
        "\n",
        "        if self.data is None:\n",
        "            print(\"Data not available. Please run fetch_and_preprocess_data first.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Training model with {self.data['X_train'].shape[0]} samples\")\n",
        "\n",
        "        # Create data loaders\n",
        "        train_dataset = TimeSeriesDataset(self.data['X_train'], self.data['y_train'])\n",
        "        val_size = int(0.2 * len(train_dataset))\n",
        "        train_size = len(train_dataset) - val_size\n",
        "\n",
        "        train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_subset,\n",
        "            batch_size=self.params['batch_size'],\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_subset,\n",
        "            batch_size=self.params['batch_size'],\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        # Define loss function and optimizer\n",
        "        # criterion = nn.MSELoss()\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.params['learning_rate'])\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=10,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "\n",
        "        # Early stopping variables\n",
        "        best_val_loss = float('inf')\n",
        "        early_stopping_counter = 0\n",
        "        best_model_state = None\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            # Train mode\n",
        "            self.model.train()\n",
        "            train_loss = 0.0\n",
        "\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs, _ = self.model(batch_X, apply_dropout=True)\n",
        "\n",
        "                loss = criterion(outputs, batch_y)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            # Validation\n",
        "            self.model.eval()\n",
        "            val_loss = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in val_loader:\n",
        "                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "\n",
        "                    outputs, _ = self.model(batch_X)\n",
        "\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    val_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                early_stopping_counter = 0\n",
        "                best_model_state = self.model.state_dict()\n",
        "\n",
        "                # Save best model\n",
        "                torch.save(\n",
        "                    self.model.state_dict(),\n",
        "                    os.path.join(f'{self.model_save_path}/models', \"best_model.pth\")\n",
        "                )\n",
        "                print(f\"Model saved at epoch {epoch+1} with validation loss: {val_loss:.6f}\")\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "                if early_stopping_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        # Load best model\n",
        "        if best_model_state is not None:\n",
        "            self.model.load_state_dict(best_model_state)\n",
        "\n",
        "        # Save final model\n",
        "        torch.save(\n",
        "            self.model.state_dict(),\n",
        "            os.path.join(f'{self.model_save_path}/models', \"final_model.pth\")\n",
        "        )\n",
        "\n",
        "        # Plot training history\n",
        "        self._plot_training_history()\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def _plot_training_history(self):\n",
        "        \"\"\"\n",
        "        Plot training history\n",
        "        \"\"\"\n",
        "        if not self.train_losses or not self.val_losses:\n",
        "            print(\"No training history available.\")\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.train_losses, label='Training Loss')\n",
        "        plt.plot(self.val_losses, label='Validation Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (MSE)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(f'{self.model_save_path}/plots', 'training_history.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        \"\"\"\n",
        "        Evaluate the model\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        metrics : dict\n",
        "            Dictionary of evaluation metrics\n",
        "        \"\"\"\n",
        "\n",
        "        if self.model is None:\n",
        "            print(\"Model not available. Please run build_model and train_model first.\")\n",
        "            return None\n",
        "\n",
        "        if self.data is None:\n",
        "            print(\"Data not available. Please run fetch_and_preprocess_data first.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Evaluating model on {self.data['X_test'].shape[0]} test samples\")\n",
        "\n",
        "        test_dataset = TimeSeriesDataset(self.data['X_test'], self.data['y_test'])\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=self.params['batch_size'],\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        # Evaluation\n",
        "        self.model.eval()\n",
        "        criterion = nn.MSELoss()\n",
        "        mae_criterion = nn.L1Loss()\n",
        "\n",
        "        test_loss = 0.0\n",
        "        test_mae = 0.0\n",
        "        preds_list = []\n",
        "        targetst_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "\n",
        "                outputs, _ = self.model(batch_X)\n",
        "\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                mae = mae_criterion(outputs, batch_y)\n",
        "\n",
        "                test_loss += loss.item() * batch_X.size(0)\n",
        "                test_mae += mae.item() * batch_X.size(0)\n",
        "\n",
        "                preds_list.extend(outputs.cpu().numpy())\n",
        "                targetst_list.extend(batch_y.cpu().numpy())\n",
        "\n",
        "        test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "        preds_list = np.array(preds_list).flatten()\n",
        "        targetst_list = np.array(targetst_list).flatten()\n",
        "\n",
        "        self.predictions = preds_list\n",
        "\n",
        "        # Calculate metrics\n",
        "        threshold = 0.5\n",
        "        y_pred = (torch.tensor(self.predictions) > threshold).int()\n",
        "        metrics = self._binary_classification_metrics(self.data['y_test'], y_pred)\n",
        "\n",
        "        self.metrics = metrics\n",
        "\n",
        "        # Save metrics\n",
        "        with open(os.path.join(f'{self.model_save_path}/results', 'evaluation_metrics.json'), 'w') as f:\n",
        "            json.dump(metrics, f, indent=4)\n",
        "\n",
        "        print(f\"Evaluation metrics: {metrics}\")\n",
        "\n",
        "        self.plot_class_distribution(self.data['y_test'])\n",
        "        self.plot_cm(self.data['y_test'], y_pred)\n",
        "        self.plot_precision_recall_curve(self.data['y_test'], self.predictions)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def predict_with_uncertainty(self, n_samples=100):\n",
        "        \"\"\"\n",
        "        Make predictions with uncertainty estimation using Monte Carlo Dropout\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_samples : int\n",
        "            Number of Monte Carlo samples\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        mean_predictions : numpy.ndarray\n",
        "            Mean predicted values\n",
        "\n",
        "        std_predictions : numpy.ndarray\n",
        "            Standard deviation of predictions\n",
        "        \"\"\"\n",
        "\n",
        "        if self.model is None:\n",
        "            print(\"Model not available. Run build_model and train_model first.\")\n",
        "            return None, None\n",
        "\n",
        "        if self.data is None:\n",
        "            print(\"Data not available. Run fetch_and_preprocess_data first.\")\n",
        "            return None, None\n",
        "\n",
        "        print(f\"Making predictions with uncertainty for {self.data['X_test'].shape[0]} samples\")\n",
        "\n",
        "        X_test_tensor = torch.tensor(self.data['X_test'], dtype=torch.float32).to(self.device)\n",
        "\n",
        "        self.model.train()  # Set to train mode to enable dropout\n",
        "\n",
        "        # Monte Carlo sampling\n",
        "        predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(n_samples):\n",
        "                outputs, _ = self.model(X_test_tensor, apply_dropout=True)\n",
        "                predictions.append(outputs.cpu().numpy())\n",
        "\n",
        "                if (i+1) % 10 == 0:\n",
        "                    print(f\"Completed {i+1}/{n_samples} Monte Carlo samples\")\n",
        "\n",
        "        # Calculate mean and standard deviation\n",
        "        predictions = np.array(predictions)\n",
        "        mean_predictions = np.mean(predictions, axis=0)\n",
        "        std_predictions = np.std(predictions, axis=0)\n",
        "\n",
        "        # Save predictions\n",
        "        np.save(os.path.join(f'{self.model_save_path}/results', 'mean_predictions.npy'), mean_predictions)\n",
        "        np.save(os.path.join(f'{self.model_save_path}/results', 'std_predictions.npy'), std_predictions)\n",
        "\n",
        "        return mean_predictions, std_predictions\n",
        "\n",
        "    def _binary_classification_metrics(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate binary classification metrics\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        y_true : numpy.ndarray\n",
        "            True labels\n",
        "\n",
        "        y_pred : numpy.ndarray\n",
        "            Predicted labels\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        metrics : dict\n",
        "            Dictionary of classification metrics\n",
        "        \"\"\"\n",
        "\n",
        "        # accuracy\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        # precision\n",
        "        precision = precision_score(y_true, y_pred)\n",
        "        # recall (sensitivity)\n",
        "        recall = recall_score(y_true, y_pred)\n",
        "        # F1-score\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "        # Compute False Positives (FP) and False Negatives (FN)\n",
        "        fp_indices = (y_pred == 1) & (y_true == 0)  # Predicted 1, actual 0\n",
        "        fn_indices = (y_pred == 0) & (y_true == 1)  # Predicted 0, actual 1\n",
        "\n",
        "        fp_rate = fp_indices.sum() / len(y_true)\n",
        "        fn_rate = fn_indices.sum() / len(y_true)\n",
        "\n",
        "        metrics = {\n",
        "            'accuracy' : float(accuracy),\n",
        "            'precision': float(precision),\n",
        "            'recall': float(recall),\n",
        "            'f1': float(f1),\n",
        "            'FP rate': float(fp_rate),\n",
        "            'FN rate': float(fn_rate)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_class_distribution(self, data):\n",
        "        \"\"\"\n",
        "        Plot the distribution of class labels in the test set.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : numpy.ndarray\n",
        "            Test labels\n",
        "        \"\"\"\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(np.array(data, dtype='bool'), bins=2)\n",
        "        plt.xlabel('Class', fontsize=14)\n",
        "        plt.ylabel('Count', fontsize=14)\n",
        "        plt.title('Distribution of Test Labels', fontsize=16, pad=20)\n",
        "        plt.xticks(ticks=[0.25, 0.75], labels=['Negative/Down', 'Positive/Up'], fontsize=12)\n",
        "\n",
        "        plt.savefig(os.path.join(f'{self.model_save_path}/plots', 'class_distribution.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def _calculate_group_percentage(self, cm=confusion_matrix):\n",
        "        \"\"\"\n",
        "        Calculate the percentage of each group in the confusion matrix.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        cm : numpy.ndarray\n",
        "            Confusion matrix\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        percentages : numpy.ndarray\n",
        "            Percentages for each group in the confusion matrix\n",
        "\n",
        "        group_a : numpy.ndarray\n",
        "            Array of group A values\n",
        "\n",
        "        group_b : numpy.ndarray\n",
        "            Array of group B values\n",
        "        \"\"\"\n",
        "\n",
        "        cm_flatten = cm.flatten()\n",
        "\n",
        "        # split into two classes\n",
        "        group_a, group_b = cm.flatten()[:2], cm.flatten()[2:]\n",
        "\n",
        "        # calculate group percentage\n",
        "        percentages = np.append(\n",
        "            group_a/np.sum(group_a), group_b/np.sum(group_b)\n",
        "            ).reshape(2,2)*100\n",
        "\n",
        "        return percentages, group_a, group_b\n",
        "\n",
        "    def _prepare_annotation_text(self, cm=confusion_matrix):\n",
        "      \"\"\"\n",
        "      Prepare the text for the confusion matrix annotations.\n",
        "\n",
        "      Parameters:\n",
        "      -----------\n",
        "      cm : numpy.ndarray\n",
        "          Confusion matrix\n",
        "\n",
        "      Returns:\n",
        "      --------\n",
        "      annotations : numpy.ndarray\n",
        "          Annotations for the confusion matrix\n",
        "\n",
        "      percentages : numpy.ndarray\n",
        "          Percentages for the confusion matrix\n",
        "      \"\"\"\n",
        "\n",
        "      percentages, group_a, group_b = self._calculate_group_percentage(cm)\n",
        "\n",
        "      total_group = np.concatenate([np.repeat(np.sum(group_a),2),\n",
        "                                    np.repeat(np.sum(group_b),2)])\n",
        "\n",
        "      group_names = ['True Negative','False Positive','False Negative','True Positive']\n",
        "      group_counts = [f\"{value}/{total}\" for value, total in zip(cm.flatten(), total_group)]\n",
        "      group_percentages = [f\"{value:.2f}%\" for value in percentages.flatten()]\n",
        "\n",
        "      annotations = np.array([f\"{name}\\n\\n{count}\\n\\n{percentage}\" for name, count, percentage in\n",
        "                zip(group_names,group_counts,group_percentages)]).reshape(2,2)\n",
        "\n",
        "      return annotations, percentages\n",
        "\n",
        "    def plot_cm(self, y_true, y_pred, figsize=(8,8)):\n",
        "        \"\"\"\n",
        "        Plot the confusion matrix.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        y_true : numpy.ndarray\n",
        "            True labels. (1 = Positive/Up, 0 = Negative/Down)\n",
        "\n",
        "        y_pred : numpy.ndarray\n",
        "            Predicted labels. (1 = Positive/Up, 0 = Negative/Down)\n",
        "\n",
        "        figsize : tuple\n",
        "            Figure size (width, height).\n",
        "        \"\"\"\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
        "\n",
        "        annotations, percentages = self._prepare_annotation_text(cm)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        sns.heatmap(percentages, cmap= \"rocket_r\", cbar_kws={'label': 'Percentage (%)'},annot=annotations, annot_kws={\"fontsize\":10},fmt='',ax=ax)\n",
        "        plt.title('Confusion Matrix', fontsize=16, pad=20)\n",
        "        plt.xlabel('Predicted', fontsize=14)\n",
        "        plt.ylabel('Actual', fontsize=14)\n",
        "        plt.xticks(ticks=[0.5, 1.5], labels=['Negative', 'Positive'], fontsize=12)\n",
        "        plt.yticks(ticks=[0.5, 1.5], labels=['Negative', 'Positive'], fontsize=12)\n",
        "        ax.xaxis.labelpad = 15\n",
        "        ax.yaxis.labelpad = 15\n",
        "\n",
        "        fig.savefig(os.path.join(f'{self.model_save_path}/plots', 'confusion_matrix.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def plot_precision_recall_curve(self, y_true, y_pred_probs):\n",
        "      \"\"\"\n",
        "      Plots the Precision-Recall Curve and calculates PR-AUC.\n",
        "\n",
        "      Parameters:\n",
        "      -----------\n",
        "      y_true : numpy.ndarray\n",
        "          True binary labels (1 = Positive/Up, 0 = Negative/Down).\n",
        "      y_pred_probs : numpy.ndarray\n",
        "          Predicted probabilities of class 1 (Up).\n",
        "      \"\"\"\n",
        "      precision, recall, thresholds = precision_recall_curve(y_true, y_pred_probs)\n",
        "      pr_auc = auc(recall, precision)\n",
        "\n",
        "      plt.figure(figsize=(8, 6))\n",
        "      plt.plot(recall, precision, marker='.', label=f'PR AUC = {pr_auc:.4f}')\n",
        "      plt.xlabel(\"Recall (Sensitivity)\", fontsize=14)\n",
        "      plt.ylabel(\"Precision (Positive Predictive Value)\", fontsize=14)\n",
        "      plt.title(\"Precision-Recall Curve\", fontsize=16, pad=20)\n",
        "      plt.legend()\n",
        "      plt.grid(True)\n",
        "      plt.savefig(os.path.join(f'{self.model_save_path}/plots', 'precision_recall_curve.png'))\n",
        "      plt.close()\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"\n",
        "        Generate a comprehensive report of the model performance\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        report : dict\n",
        "            Dictionary containing the report\n",
        "        \"\"\"\n",
        "\n",
        "        if self.data is None or self.model is None or self.metrics is None:\n",
        "            print(\"Data, model, or metrics not available. Please complete the training and evaluation first.\")\n",
        "            return None\n",
        "\n",
        "        print(\"Generating comprehensive report\")\n",
        "\n",
        "        # Create report\n",
        "        report = {\n",
        "            'model_summary': {\n",
        "                'ticker' : self.ticker_name,\n",
        "                'ticker_symbol' : self.ticker,\n",
        "                'input_shape': (self.data['X_train'].shape[1], self.data['X_train'].shape[2]),\n",
        "                'hyperparameters': self.params,\n",
        "                'training_samples': self.data['X_train'].shape[0],\n",
        "                'test_samples': self.data['X_test'].shape[0]\n",
        "            },\n",
        "            'evaluation_metrics': self.metrics,\n",
        "            'training_period': {\n",
        "                'start_date': self.start_date,\n",
        "                'end_date': self.end_date\n",
        "            },\n",
        "            'features_used': self.data['metadata']['features']\n",
        "        }\n",
        "\n",
        "        # Save report to file\n",
        "        with open(os.path.join(f'{self.model_save_path}/results', 'model_report.json'), 'w') as f:\n",
        "            json.dump(report, f, indent=4)\n",
        "\n",
        "        print(f\"Report generated and saved to {self.model_save_path}/results/model_report.json\")\n",
        "        return report"
      ],
      "metadata": {
        "id": "56ThcLB87uN7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to train and evaluate the prediction model\n",
        "    \"\"\"\n",
        "    print(\"Starting prediction model training and evaluation with PyTorch\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "\n",
        "    # Create directories\n",
        "    model_save_path =f'./model/{timestamp}'\n",
        "\n",
        "    os.makedirs(f'{model_save_path}', exist_ok=True)\n",
        "    os.makedirs(f'{model_save_path}/data', exist_ok=True)\n",
        "    os.makedirs(f'{model_save_path}/models', exist_ok=True)\n",
        "    os.makedirs(f'{model_save_path}/results', exist_ok=True)\n",
        "    os.makedirs(f'{model_save_path}/plots', exist_ok=True)\n",
        "\n",
        "    # Optimized hyperparameters\n",
        "    optimized_params = {\n",
        "        'dropout_rate': 0.2,\n",
        "        'learning_rate': 0.0005,\n",
        "        'batch_size': 32,\n",
        "        'hidden_size_1': 128,\n",
        "        'hidden_size_2': 64,\n",
        "        'hidden_size_3': 32\n",
        "    }\n",
        "\n",
        "    # name and ticker\n",
        "    ticker_info = {'EURUSD': 'EURUSD=X'}\n",
        "\n",
        "    # date range\n",
        "    start_date = '2010-01-01'\n",
        "    end_date = None\n",
        "\n",
        "    # Fetch and preprocess data\n",
        "    data_handler = FinancialDataHandler(\n",
        "        ticker_info=ticker_info,\n",
        "        start_date=start_date,\n",
        "        end_date=end_date,\n",
        "        window_size=30,\n",
        "        train_split=0.8,\n",
        "        model_save_path=model_save_path\n",
        "    )\n",
        "\n",
        "    processed_data = data_handler.fetch_and_preprocess_data()\n",
        "\n",
        "    # Initialize model with optimized parameters\n",
        "    model = PredictionModel(\n",
        "        ticker_info,\n",
        "        processed_data,\n",
        "        start_date,\n",
        "        end_date,\n",
        "        device,\n",
        "        model_save_path,\n",
        "        optimized_params=optimized_params\n",
        "    )\n",
        "\n",
        "    # Build model\n",
        "    model.build_model()\n",
        "\n",
        "    # Train model\n",
        "    model.train_model(epochs=100, patience=20)\n",
        "\n",
        "    # Evaluate model\n",
        "    model.evaluate_model()\n",
        "\n",
        "    # Generate report\n",
        "    model.generate_report()\n",
        "\n",
        "    print(f\"{list(ticker_info.keys())[0]} prediction model training and evaluation completed\")\n",
        "    print(f\"Results and visualizations saved to {model_save_path}/ directory\")"
      ],
      "metadata": {
        "id": "2TkybLXfDxMC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCVEjs0iKFr8",
        "outputId": "222674ae-7484-45a7-cc0b-d4b394db0dd4",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting prediction model training and evaluation with PyTorch\n",
            "Fetching EURUSD data from 2010-01-01 to 2025-03-31\n",
            "YF.download() has changed argument auto_adjust default to True\n",
            "Raw data saved to ./model/2025-03-31_09-25-20/data/EURUSD=X_raw.csv\n",
            "Calculating technical indicators\n",
            "Technical indicators calculated\n",
            "Preprocessing data\n",
            "Preprocessed data saved. X_train shape: (3136, 30, 20), y_train shape: (3136,)\n",
            "X_test shape: (785, 30, 20), y_test shape: (785,)\n",
            "Initialized Model with parameters: {'dropout_rate': 0.2, 'learning_rate': 0.0005, 'batch_size': 32, 'hidden_size_1': 128, 'hidden_size_2': 64, 'hidden_size_3': 32}\n",
            "Building stacked LSTM model with input size 20\n",
            "StackedLSTMModel(\n",
            "  (lstm1): LSTM(20, 128, batch_first=True)\n",
            "  (dropout1): Dropout(p=0.2, inplace=False)\n",
            "  (lstm2): LSTM(128, 64, batch_first=True)\n",
            "  (dropout2): Dropout(p=0.2, inplace=False)\n",
            "  (attention): AttentionLayer(\n",
            "    (attention): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            "  (lstm3): LSTM(128, 32, batch_first=True)\n",
            "  (dropout3): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 147298\n",
            "Training model with 3136 samples\n",
            "Epoch 1/100 - Train Loss: 0.641656, Val Loss: 0.635365\n",
            "Model saved at epoch 1 with validation loss: 0.635365\n",
            "Epoch 2/100 - Train Loss: 0.450803, Val Loss: 0.446137\n",
            "Model saved at epoch 2 with validation loss: 0.446137\n",
            "Epoch 3/100 - Train Loss: 0.294964, Val Loss: 0.344433\n",
            "Model saved at epoch 3 with validation loss: 0.344433\n",
            "Epoch 4/100 - Train Loss: 0.258396, Val Loss: 0.329439\n",
            "Model saved at epoch 4 with validation loss: 0.329439\n",
            "Epoch 5/100 - Train Loss: 0.255108, Val Loss: 0.390516\n",
            "Epoch 6/100 - Train Loss: 0.199188, Val Loss: 0.308481\n",
            "Model saved at epoch 6 with validation loss: 0.308481\n",
            "Epoch 7/100 - Train Loss: 0.192272, Val Loss: 0.255679\n",
            "Model saved at epoch 7 with validation loss: 0.255679\n",
            "Epoch 8/100 - Train Loss: 0.186057, Val Loss: 0.339591\n",
            "Epoch 9/100 - Train Loss: 0.177033, Val Loss: 0.333027\n",
            "Epoch 10/100 - Train Loss: 0.187219, Val Loss: 0.307197\n",
            "Epoch 11/100 - Train Loss: 0.142026, Val Loss: 0.364816\n",
            "Epoch 12/100 - Train Loss: 0.180101, Val Loss: 0.382613\n",
            "Epoch 13/100 - Train Loss: 0.131681, Val Loss: 0.301570\n",
            "Epoch 14/100 - Train Loss: 0.126830, Val Loss: 0.292094\n",
            "Epoch 15/100 - Train Loss: 0.152134, Val Loss: 0.320516\n",
            "Epoch 16/100 - Train Loss: 0.145612, Val Loss: 0.266518\n",
            "Epoch 17/100 - Train Loss: 0.115879, Val Loss: 0.183256\n",
            "Model saved at epoch 17 with validation loss: 0.183256\n",
            "Epoch 18/100 - Train Loss: 0.127025, Val Loss: 0.181610\n",
            "Model saved at epoch 18 with validation loss: 0.181610\n",
            "Epoch 19/100 - Train Loss: 0.108489, Val Loss: 0.243900\n",
            "Epoch 20/100 - Train Loss: 0.116957, Val Loss: 0.293886\n",
            "Epoch 21/100 - Train Loss: 0.101860, Val Loss: 0.227245\n",
            "Epoch 22/100 - Train Loss: 0.129150, Val Loss: 0.279303\n",
            "Epoch 23/100 - Train Loss: 0.092323, Val Loss: 0.221654\n",
            "Epoch 24/100 - Train Loss: 0.100655, Val Loss: 0.370873\n",
            "Epoch 25/100 - Train Loss: 0.104652, Val Loss: 0.158727\n",
            "Model saved at epoch 25 with validation loss: 0.158727\n",
            "Epoch 26/100 - Train Loss: 0.094910, Val Loss: 0.224007\n",
            "Epoch 27/100 - Train Loss: 0.089307, Val Loss: 0.200638\n",
            "Epoch 28/100 - Train Loss: 0.107434, Val Loss: 0.214803\n",
            "Epoch 29/100 - Train Loss: 0.101031, Val Loss: 0.200149\n",
            "Epoch 30/100 - Train Loss: 0.104192, Val Loss: 0.147280\n",
            "Model saved at epoch 30 with validation loss: 0.147280\n",
            "Epoch 31/100 - Train Loss: 0.089939, Val Loss: 0.236166\n",
            "Epoch 32/100 - Train Loss: 0.082751, Val Loss: 0.190484\n",
            "Epoch 33/100 - Train Loss: 0.096372, Val Loss: 0.136085\n",
            "Model saved at epoch 33 with validation loss: 0.136085\n",
            "Epoch 34/100 - Train Loss: 0.089383, Val Loss: 0.227342\n",
            "Epoch 35/100 - Train Loss: 0.093236, Val Loss: 0.166662\n",
            "Epoch 36/100 - Train Loss: 0.077675, Val Loss: 0.241766\n",
            "Epoch 37/100 - Train Loss: 0.093125, Val Loss: 0.451410\n",
            "Epoch 38/100 - Train Loss: 0.109329, Val Loss: 0.216337\n",
            "Epoch 39/100 - Train Loss: 0.088632, Val Loss: 0.134964\n",
            "Model saved at epoch 39 with validation loss: 0.134964\n",
            "Epoch 40/100 - Train Loss: 0.101582, Val Loss: 0.149972\n",
            "Epoch 41/100 - Train Loss: 0.120624, Val Loss: 0.167343\n",
            "Epoch 42/100 - Train Loss: 0.077782, Val Loss: 0.159123\n",
            "Epoch 43/100 - Train Loss: 0.082385, Val Loss: 0.132311\n",
            "Model saved at epoch 43 with validation loss: 0.132311\n",
            "Epoch 44/100 - Train Loss: 0.096197, Val Loss: 0.161917\n",
            "Epoch 45/100 - Train Loss: 0.089016, Val Loss: 0.128968\n",
            "Model saved at epoch 45 with validation loss: 0.128968\n",
            "Epoch 46/100 - Train Loss: 0.086764, Val Loss: 0.158161\n",
            "Epoch 47/100 - Train Loss: 0.091557, Val Loss: 0.258538\n",
            "Epoch 48/100 - Train Loss: 0.097501, Val Loss: 0.268845\n",
            "Epoch 49/100 - Train Loss: 0.078996, Val Loss: 0.157355\n",
            "Epoch 50/100 - Train Loss: 0.073775, Val Loss: 0.150187\n",
            "Epoch 51/100 - Train Loss: 0.090805, Val Loss: 0.156026\n",
            "Epoch 52/100 - Train Loss: 0.073576, Val Loss: 0.165746\n",
            "Epoch 53/100 - Train Loss: 0.071209, Val Loss: 0.198984\n",
            "Epoch 54/100 - Train Loss: 0.083421, Val Loss: 0.246655\n",
            "Epoch 55/100 - Train Loss: 0.082344, Val Loss: 0.195666\n",
            "Epoch 56/100 - Train Loss: 0.076673, Val Loss: 0.166161\n",
            "Epoch 57/100 - Train Loss: 0.057694, Val Loss: 0.117212\n",
            "Model saved at epoch 57 with validation loss: 0.117212\n",
            "Epoch 58/100 - Train Loss: 0.063163, Val Loss: 0.132185\n",
            "Epoch 59/100 - Train Loss: 0.063337, Val Loss: 0.140681\n",
            "Epoch 60/100 - Train Loss: 0.063286, Val Loss: 0.112172\n",
            "Model saved at epoch 60 with validation loss: 0.112172\n",
            "Epoch 61/100 - Train Loss: 0.055895, Val Loss: 0.160200\n",
            "Epoch 62/100 - Train Loss: 0.060647, Val Loss: 0.125589\n",
            "Epoch 63/100 - Train Loss: 0.057913, Val Loss: 0.208495\n",
            "Epoch 64/100 - Train Loss: 0.061979, Val Loss: 0.145905\n",
            "Epoch 65/100 - Train Loss: 0.069865, Val Loss: 0.104430\n",
            "Model saved at epoch 65 with validation loss: 0.104430\n",
            "Epoch 66/100 - Train Loss: 0.060309, Val Loss: 0.139451\n",
            "Epoch 67/100 - Train Loss: 0.070122, Val Loss: 0.103419\n",
            "Model saved at epoch 67 with validation loss: 0.103419\n",
            "Epoch 68/100 - Train Loss: 0.072589, Val Loss: 0.124977\n",
            "Epoch 69/100 - Train Loss: 0.062778, Val Loss: 0.127779\n",
            "Epoch 70/100 - Train Loss: 0.057234, Val Loss: 0.116221\n",
            "Epoch 71/100 - Train Loss: 0.054089, Val Loss: 0.134743\n",
            "Epoch 72/100 - Train Loss: 0.055900, Val Loss: 0.121097\n",
            "Epoch 73/100 - Train Loss: 0.056472, Val Loss: 0.128868\n",
            "Epoch 74/100 - Train Loss: 0.066624, Val Loss: 0.168731\n",
            "Epoch 75/100 - Train Loss: 0.064062, Val Loss: 0.143523\n",
            "Epoch 76/100 - Train Loss: 0.067631, Val Loss: 0.151560\n",
            "Epoch 77/100 - Train Loss: 0.057760, Val Loss: 0.107861\n",
            "Epoch 78/100 - Train Loss: 0.060263, Val Loss: 0.163048\n",
            "Epoch 79/100 - Train Loss: 0.056315, Val Loss: 0.155487\n",
            "Epoch 80/100 - Train Loss: 0.048672, Val Loss: 0.152345\n",
            "Epoch 81/100 - Train Loss: 0.050933, Val Loss: 0.111242\n",
            "Epoch 82/100 - Train Loss: 0.045390, Val Loss: 0.104240\n",
            "Epoch 83/100 - Train Loss: 0.048215, Val Loss: 0.140516\n",
            "Epoch 84/100 - Train Loss: 0.046425, Val Loss: 0.133168\n",
            "Epoch 85/100 - Train Loss: 0.047673, Val Loss: 0.097109\n",
            "Model saved at epoch 85 with validation loss: 0.097109\n",
            "Epoch 86/100 - Train Loss: 0.049410, Val Loss: 0.108746\n",
            "Epoch 87/100 - Train Loss: 0.050283, Val Loss: 0.116899\n",
            "Epoch 88/100 - Train Loss: 0.047106, Val Loss: 0.110473\n",
            "Epoch 89/100 - Train Loss: 0.046742, Val Loss: 0.147793\n",
            "Epoch 90/100 - Train Loss: 0.051543, Val Loss: 0.125650\n",
            "Epoch 91/100 - Train Loss: 0.047344, Val Loss: 0.177245\n",
            "Epoch 92/100 - Train Loss: 0.047178, Val Loss: 0.125541\n",
            "Epoch 93/100 - Train Loss: 0.047562, Val Loss: 0.121265\n",
            "Epoch 94/100 - Train Loss: 0.050324, Val Loss: 0.119805\n",
            "Epoch 95/100 - Train Loss: 0.046508, Val Loss: 0.152742\n",
            "Epoch 96/100 - Train Loss: 0.047302, Val Loss: 0.114845\n",
            "Epoch 97/100 - Train Loss: 0.046894, Val Loss: 0.133871\n",
            "Epoch 98/100 - Train Loss: 0.045068, Val Loss: 0.121823\n",
            "Epoch 99/100 - Train Loss: 0.040941, Val Loss: 0.144613\n",
            "Epoch 100/100 - Train Loss: 0.041742, Val Loss: 0.184951\n",
            "Evaluating model on 785 test samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-213b8846f73a>:408: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  fp_indices = (y_pred == 1) & (y_true == 0)  # Predicted 1, actual 0\n",
            "<ipython-input-6-213b8846f73a>:409: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  fn_indices = (y_pred == 0) & (y_true == 1)  # Predicted 0, actual 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation metrics: {'accuracy': 0.9477707006369427, 'precision': 1.0, 'recall': 0.8845070422535212, 'f1': 0.9387144992526159, 'FP rate': 0.0, 'FN rate': 0.052229300141334534}\n",
            "Generating comprehensive report\n",
            "Report generated and saved to ./model/2025-03-31_09-25-20/results/model_report.json\n",
            "EURUSD prediction model training and evaluation completed\n",
            "Results and visualizations saved to ./model/2025-03-31_09-25-20/ directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1jiPlEIxhQ76"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}