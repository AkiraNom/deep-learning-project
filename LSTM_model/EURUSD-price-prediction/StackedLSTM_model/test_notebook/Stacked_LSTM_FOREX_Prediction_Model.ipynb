{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zroL1Om3keT9"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import scipy.stats as stats\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)"
      ],
      "metadata": {
        "id": "WuZUj3kwk68M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for time series data\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "u-xzz4Uy-pAo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FinancialDataHandler:\n",
        "    def __init__(self,\n",
        "                 ticker_info={'EURUSD':'EURUSD=X'},\n",
        "                 start_date='2020-01-01',\n",
        "                 end_date=None,\n",
        "                 window_size=30,\n",
        "                 train_split=0.8,\n",
        "                 model_save_path='.'):\n",
        "        \"\"\"\n",
        "        Initialize the model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        ticker : dict\n",
        "            Dictionary containing ticker name and ticker symbol\n",
        "        start_date : str\n",
        "            Start date for data collection (YYYY-MM-DD)\n",
        "        end_date : str\n",
        "            End date for data collection (YYYY-MM-DD), defaults to current date\n",
        "        window_size : int\n",
        "            Size of the input window (how many past days to use)\n",
        "        train_split : float\n",
        "            Proportion of data to use for training, defaults to 0.8\n",
        "        model_save_path : str\n",
        "            Path to save the model, defaults to current directory\n",
        "\n",
        "        \"\"\"\n",
        "        self.ticker_name = list(ticker_info.keys())[0]\n",
        "        self.ticker = list(ticker_info.values())[0]\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')\n",
        "        self.window_size = window_size\n",
        "        self.train_split = train_split\n",
        "        self.model_save_path = model_save_path\n",
        "\n",
        "        # data containers\n",
        "        self.raw_data = None\n",
        "        self.data = None\n",
        "\n",
        "    def fetch_data(self):\n",
        "        \"\"\"\n",
        "        Fetch data from Yahoo Finance and save the raw data to a CSV file\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            Raw data\n",
        "        \"\"\"\n",
        "        print(f\"Fetching {self.ticker_name} data from {self.start_date} to {self.end_date}\")\n",
        "\n",
        "        try:\n",
        "            # Yahoo Finance ticker for EUR/USD\n",
        "            self.raw_data = yf.download(\n",
        "                self.ticker,\n",
        "                start=self.start_date,\n",
        "                end=self.end_date,\n",
        "                interval=\"1d\",\n",
        "                progress=False\n",
        "            )\n",
        "\n",
        "            if self.raw_data.empty:\n",
        "                print(f\"No data found for {self.ticker}\")\n",
        "                return None\n",
        "\n",
        "            # Save raw data to CSV\n",
        "            csv_path = os.path.join(f'{self.model_save_path}/data', f\"{self.ticker}_raw.csv\")\n",
        "            self.raw_data.to_csv(csv_path)\n",
        "            print(f\"Raw data saved to {csv_path}\")\n",
        "\n",
        "            return self.raw_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {self.ticker} data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_and_preprocess_data(self):\n",
        "        \"\"\"\n",
        "        Fetch data and calculate technical indicators, then preprocess the data\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            Preprocessed data\n",
        "        \"\"\"\n",
        "\n",
        "        # Fetch data\n",
        "        data = self.fetch_data()\n",
        "\n",
        "        # Calculate technical indicators\n",
        "        df = self._calculate_technical_indicators(data)\n",
        "\n",
        "        # Preprocess data\n",
        "        processed_data = self._preprocess_data(df)\n",
        "\n",
        "        self.data = processed_data\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def _calculate_technical_indicators(self, df):\n",
        "        \"\"\"\n",
        "        Calculate technical indicators\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame containing forex data\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            DataFrame with added technical indicators\n",
        "        \"\"\"\n",
        "        print(\"Calculating technical indicators\")\n",
        "\n",
        "        # Make a copy to avoid modifying the original dataframe\n",
        "        df = df.copy()\n",
        "\n",
        "        # Moving averages\n",
        "        df['MA5'] = df['Close'].rolling(window=5).mean()\n",
        "        df['MA10'] = df['Close'].rolling(window=10).mean()\n",
        "        df['MA20'] = df['Close'].rolling(window=20).mean()\n",
        "\n",
        "        # Bollinger Bands (20-day, 2 standard deviations)\n",
        "        df['BB_middle'] = df['Close'].rolling(window=20).mean()\n",
        "        df['BB_std'] = df['Close'].rolling(window=20).std()\n",
        "        df['BB_upper'] = df['BB_middle'] + 2 * df['BB_std']\n",
        "        df['BB_lower'] = df['BB_middle'] - 2 * df['BB_std']\n",
        "\n",
        "        # RSI (Relative Strength Index) - 14 days\n",
        "        delta = df['Close'].diff()\n",
        "        gain = delta.where(delta > 0, 0)\n",
        "        loss = -delta.where(delta < 0, 0)\n",
        "        avg_gain = gain.rolling(window=14).mean()\n",
        "        avg_loss = loss.rolling(window=14).mean()\n",
        "        rs = avg_gain / avg_loss\n",
        "        df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "        # MACD (Moving Average Convergence Divergence)\n",
        "        df['EMA12'] = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "        df['EMA26'] = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "        df['MACD'] = df['EMA12'] - df['EMA26']\n",
        "        df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
        "        df['MACD_hist'] = df['MACD'] - df['MACD_signal']\n",
        "\n",
        "        # Price changes\n",
        "        df['Price_Change'] = df['Close'].pct_change()\n",
        "        df['Price_Change_5d'] = df['Close'].pct_change(periods=5)\n",
        "\n",
        "        # Volatility indicators\n",
        "        df['HL_Diff'] = df['High'] - df['Low']\n",
        "        df['HL_Diff_Pct'] = (df['High'] - df['Low']) / df['Low']\n",
        "\n",
        "        # ATR (Average True Range) - 14 days\n",
        "        high_low = df['High'] - df['Low']\n",
        "        high_close = (df['High'] - df['Close'].shift()).abs()\n",
        "        low_close = (df['Low'] - df['Close'].shift()).abs()\n",
        "        ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
        "        true_range = ranges.max(axis=1)\n",
        "        df['ATR'] = true_range.rolling(14).mean()\n",
        "\n",
        "        # Drop NaN values\n",
        "        df = df.dropna()\n",
        "\n",
        "        print(\"Technical indicators calculated\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _preprocess_data(self, df):\n",
        "        \"\"\"\n",
        "        Preprocess data for LSTM model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame containing forex data with technical indicators\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Dictionary containing preprocessed data\n",
        "        \"\"\"\n",
        "        print(\"Preprocessing data\")\n",
        "\n",
        "        # Select features\n",
        "        features = [\n",
        "            'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "            'MA5', 'MA10', 'MA20',\n",
        "            'BB_upper', 'BB_middle', 'BB_lower',\n",
        "            'RSI', 'MACD', 'MACD_signal', 'MACD_hist',\n",
        "            'Price_Change', 'Price_Change_5d',\n",
        "            'HL_Diff', 'HL_Diff_Pct', 'ATR'\n",
        "        ]\n",
        "\n",
        "        # Scale the features\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        scaled_data = scaler.fit_transform(df[features])\n",
        "        scaled_df = pd.DataFrame(scaled_data, index=df.index, columns=features)\n",
        "\n",
        "        # Create sequences for LSTM\n",
        "        X, y = [], []\n",
        "        for i in range(len(scaled_df) - self.window_size):\n",
        "            X.append(scaled_df.iloc[i:i+self.window_size].values)\n",
        "\n",
        "            # target: close price after the window_size\n",
        "            close_idx = features.index('Close')\n",
        "            y.append(scaled_df.iloc[i+self.window_size, close_idx])\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        # Split into train and test sets\n",
        "        split_idx = int(len(X) * self.train_split)\n",
        "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "        # Save dates for visualization\n",
        "        dates = df.index[self.window_size:].tolist()\n",
        "        train_dates = dates[:split_idx]\n",
        "        test_dates = dates[split_idx:]\n",
        "\n",
        "        # Save preprocessed data\n",
        "        np.save(os.path.join(f'{self.model_save_path}/data', \"X_train.npy\"), X_train)\n",
        "        np.save(os.path.join(f'{self.model_save_path}/data', \"y_train.npy\"), y_train)\n",
        "        np.save(os.path.join(f'{self.model_save_path}/data', \"X_test.npy\"), X_test)\n",
        "        np.save(os.path.join(f'{self.model_save_path}/data', \"y_test.npy\"), y_test)\n",
        "\n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            'features': features,\n",
        "            'close_idx': close_idx,\n",
        "            'train_dates': [d.strftime('%Y-%m-%d') for d in train_dates],\n",
        "            'test_dates': [d.strftime('%Y-%m-%d') for d in test_dates]\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(f'{self.model_save_path}/data', \"metadata.pth\"), 'wb') as f:\n",
        "            pickle.dump(metadata, f)\n",
        "\n",
        "        with open(os.path.join(f'{self.model_save_path}/data', \"scaler.pth\"), 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "\n",
        "        print(f\"Preprocessed data saved. X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "        return {\n",
        "            'X_train': X_train,\n",
        "            'y_train': y_train,\n",
        "            'X_test': X_test,\n",
        "            'y_test': y_test,\n",
        "            'metadata': metadata,\n",
        "            'scaler': scaler,\n",
        "            'raw_df': df\n",
        "        }"
      ],
      "metadata": {
        "id": "p4JwZPlGwYMV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention mechanism for LSTM\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attention = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        # lstm_output shape: (batch_size, seq_len, hidden_size)\n",
        "        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)\n",
        "        # attention_weights shape: (batch_size, seq_len, 1)\n",
        "\n",
        "        context_vector = torch.sum(attention_weights * lstm_output, dim=1)\n",
        "        # context_vector shape: (batch_size, hidden_size)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class StackedLSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Stacked LSTM model with attention mechanism\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size_1=128, hidden_size_2=64, hidden_size_3=32,\n",
        "                 dropout_rate=0.2, output_size=1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size_1 = hidden_size_1\n",
        "        self.hidden_size_2 = hidden_size_2\n",
        "        self.hidden_size_3 = hidden_size_3\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # First LSTM layer\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_size_1, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Second LSTM layer\n",
        "        self.lstm2 = nn.LSTM(hidden_size_1, hidden_size_2, batch_first=True)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = AttentionLayer(hidden_size_2)\n",
        "\n",
        "        # Third LSTM layer\n",
        "        self.lstm3 = nn.LSTM(hidden_size_2 * 2, hidden_size_3, batch_first=True)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_size_3, output_size)\n",
        "\n",
        "    def forward(self, x, apply_dropout=False):\n",
        "        # x shape: (batch_size, seq_len, input_size)\n",
        "\n",
        "        # First LSTM layer\n",
        "        lstm1_out, _ = self.lstm1(x)\n",
        "        if apply_dropout:\n",
        "            lstm1_out = self.dropout1(lstm1_out)\n",
        "        else:\n",
        "            lstm1_out = lstm1_out * (1 - self.dropout_rate)\n",
        "\n",
        "        # Second LSTM layer\n",
        "        lstm2_out, _ = self.lstm2(lstm1_out)\n",
        "        if apply_dropout:\n",
        "            lstm2_out = self.dropout2(lstm2_out)\n",
        "        else:\n",
        "            lstm2_out = lstm2_out * (1 - self.dropout_rate)\n",
        "\n",
        "        # Attention mechanism\n",
        "        context_vector, attention_weights = self.attention(lstm2_out)\n",
        "        context_vector_expanded = context_vector.unsqueeze(1).expand(-1, lstm2_out.size(1), -1)\n",
        "        combined = torch.cat((lstm2_out, context_vector_expanded), dim=2)\n",
        "\n",
        "        # Third LSTM layer\n",
        "        lstm3_out, _ = self.lstm3(combined)\n",
        "        if apply_dropout:\n",
        "            lstm3_out = self.dropout3(lstm3_out)\n",
        "        else:\n",
        "            lstm3_out = lstm3_out * (1 - self.dropout_rate)\n",
        "\n",
        "        # ooutput at last time step\n",
        "        last_time_step = lstm3_out[:, -1, :]\n",
        "\n",
        "        # Output layer\n",
        "        output = self.fc(last_time_step)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "5a_9owzy6aig"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictionModel:\n",
        "\n",
        "    def __init__(self, ticker, data, start_date, end_date, device, save_model_path= '.',optimized_params=None):\n",
        "        \"\"\"\n",
        "        Initialize the model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        ticker : dict\n",
        "            Dictionary containing ticker name and ticker symbol (yfinance)\n",
        "        device : torch.device\n",
        "            Device to run the model on\n",
        "        data : dict\n",
        "            Dictionary containing preprocessed data\n",
        "        date : tuple\n",
        "            Tuple containing start and end dates\n",
        "        save_model_path : str\n",
        "            Path to save the model, defaults to current directory\n",
        "        optimized_params : dict\n",
        "            Dictionary containing model parameters\n",
        "        \"\"\"\n",
        "\n",
        "        self.ticker_name = list(ticker.keys())[0]\n",
        "        self.ticker = list(ticker.values())[0]\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')\n",
        "        self.data = data\n",
        "        self.device = device\n",
        "        self.model_save_path = save_model_path\n",
        "\n",
        "        # Initialize data containers\n",
        "        self.model = None\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = None\n",
        "        self.metrics = None\n",
        "\n",
        "        # Default hyperparameters\n",
        "        self.params = {\n",
        "            'dropout_rate': 0.2,\n",
        "            'learning_rate': 0.0005,\n",
        "            'batch_size': 32,\n",
        "            'hidden_size_1': 128,\n",
        "            'hidden_size_2': 64,\n",
        "            'hidden_size_3': 32\n",
        "        }\n",
        "\n",
        "        # Update params with opitmized params if provided\n",
        "        if optimized_params:\n",
        "          self.params.update(optimized_params)\n",
        "\n",
        "        print(f\"Initialized Model with parameters: {self.params}\")\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build the stacked LSTM model with optimized hyperparameters\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        model : StackedLSTMModel\n",
        "            Built PyTorch model\n",
        "        \"\"\"\n",
        "        if self.data is None:\n",
        "            print(\"Data not available. Please run fetch_and_preprocess_data first.\")\n",
        "            return None\n",
        "\n",
        "        input_size = self.data['X_train'].shape[2]\n",
        "        print(f\"Building stacked LSTM model with input size {input_size}\")\n",
        "\n",
        "        # Create model\n",
        "        model = StackedLSTMModel(\n",
        "            input_size=input_size,\n",
        "            hidden_size_1=self.params['hidden_size_1'],\n",
        "            hidden_size_2=self.params['hidden_size_2'],\n",
        "            hidden_size_3=self.params['hidden_size_3'],\n",
        "            dropout_rate=self.params['dropout_rate']\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Print model summary\n",
        "        print(model)\n",
        "        print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_model(self, epochs=100, patience=20):\n",
        "        \"\"\"\n",
        "        Train the model with optimized hyperparameters\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        epochs : int\n",
        "            Maximum number of epochs\n",
        "        patience : int\n",
        "            Patience for early stopping\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        model : StackedLSTMModel\n",
        "            Trained PyTorch model\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"Model not built. Please run build_model first.\")\n",
        "            return None\n",
        "\n",
        "        if self.data is None:\n",
        "            print(\"Data not available. Please run fetch_and_preprocess_data first.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Training model with {self.data['X_train'].shape[0]} samples\")\n",
        "\n",
        "        # Create data loaders\n",
        "        train_dataset = TimeSeriesDataset(self.data['X_train'], self.data['y_train'])\n",
        "        val_size = int(0.2 * len(train_dataset))\n",
        "        train_size = len(train_dataset) - val_size\n",
        "\n",
        "        train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_subset,\n",
        "            batch_size=self.params['batch_size'],\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_subset,\n",
        "            batch_size=self.params['batch_size'],\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        # Define loss function and optimizer\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.params['learning_rate'])\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=10,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "\n",
        "        # Early stopping variables\n",
        "        best_val_loss = float('inf')\n",
        "        early_stopping_counter = 0\n",
        "        best_model_state = None\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            # Train mode\n",
        "            self.model.train()\n",
        "            train_loss = 0.0\n",
        "\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs, _ = self.model(batch_X, apply_dropout=True)\n",
        "\n",
        "                loss = criterion(outputs, batch_y)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "            train_loss = train_loss / len(train_loader.dataset)\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            # Validation\n",
        "            self.model.eval()\n",
        "            val_loss = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in val_loader:\n",
        "                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "\n",
        "                    outputs, _ = self.model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    val_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                early_stopping_counter = 0\n",
        "                best_model_state = self.model.state_dict()\n",
        "\n",
        "                # Save best model\n",
        "                torch.save(\n",
        "                    self.model.state_dict(),\n",
        "                    os.path.join(f'{self.model_save_path}/models', \"best_model.pth\")\n",
        "                )\n",
        "                print(f\"Model saved at epoch {epoch+1} with validation loss: {val_loss:.6f}\")\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "                if early_stopping_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        # Load best model\n",
        "        if best_model_state is not None:\n",
        "            self.model.load_state_dict(best_model_state)\n",
        "\n",
        "        # Save final model\n",
        "        torch.save(\n",
        "            self.model.state_dict(),\n",
        "            os.path.join(f'{self.model_save_path}/models', \"final_model.pth\")\n",
        "        )\n",
        "\n",
        "        # Plot training history\n",
        "        self._plot_training_history()\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def _plot_training_history(self):\n",
        "        \"\"\"\n",
        "        Plot training history\n",
        "        \"\"\"\n",
        "        if not self.train_losses or not self.val_losses:\n",
        "            print(\"No training history available.\")\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.train_losses, label='Training Loss')\n",
        "        plt.plot(self.val_losses, label='Validation Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (MSE)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(f'{self.model_save_path}/plots', 'training_history.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        \"\"\"\n",
        "        Evaluate the model\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        metrics : dict\n",
        "            Dictionary of evaluation metrics\n",
        "        \"\"\"\n",
        "\n",
        "        if self.model is None:\n",
        "            print(\"Model not available. Please run build_model and train_model first.\")\n",
        "            return None\n",
        "\n",
        "        if self.data is None:\n",
        "            print(\"Data not available. Please run fetch_and_preprocess_data first.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Evaluating model on {self.data['X_test'].shape[0]} test samples\")\n",
        "\n",
        "        test_dataset = TimeSeriesDataset(self.data['X_test'], self.data['y_test'])\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=self.params['batch_size'],\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        # Evaluation\n",
        "        self.model.eval()\n",
        "        criterion = nn.MSELoss()\n",
        "        mae_criterion = nn.L1Loss()\n",
        "\n",
        "        test_loss = 0.0\n",
        "        test_mae = 0.0\n",
        "        preds_list = []\n",
        "        targetst_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in test_loader:\n",
        "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "\n",
        "                outputs, _ = self.model(batch_X)\n",
        "\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                mae = mae_criterion(outputs, batch_y)\n",
        "\n",
        "                test_loss += loss.item() * batch_X.size(0)\n",
        "                test_mae += mae.item() * batch_X.size(0)\n",
        "\n",
        "                preds_list.extend(outputs.cpu().numpy())\n",
        "                targetst_list.extend(batch_y.cpu().numpy())\n",
        "\n",
        "        test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "        preds_list = np.array(preds_list).flatten()\n",
        "        targetst_list = np.array(targetst_list).flatten()\n",
        "\n",
        "        self.predictions = preds_list\n",
        "\n",
        "        # Calculate metrics\n",
        "        test_mae = test_mae / len(test_loader.dataset)\n",
        "        rmse = np.sqrt(test_loss)\n",
        "\n",
        "        # Calculate direction accuracy\n",
        "        direction_actual = np.diff(targetst_list)\n",
        "        direction_pred = np.diff(preds_list)\n",
        "        direction_accuracy = np.mean((direction_actual > 0) == (direction_pred > 0))\n",
        "\n",
        "        # Calculate R-squared\n",
        "        r2 = r2_score(targetst_list, preds_list)\n",
        "\n",
        "        metrics = {\n",
        "            'mse': float(test_loss),\n",
        "            'mae': float(test_mae),\n",
        "            'rmse': float(rmse),\n",
        "            'direction_accuracy': float(direction_accuracy),\n",
        "            'r2': float(r2)\n",
        "        }\n",
        "\n",
        "        self.metrics = metrics\n",
        "\n",
        "        # Save metrics\n",
        "        with open(os.path.join(f'{self.model_save_path}/results', 'evaluation_metrics.json'), 'w') as f:\n",
        "            json.dump(metrics, f, indent=4)\n",
        "\n",
        "        print(f\"Evaluation metrics: {metrics}\")\n",
        "\n",
        "        self._residual_analysis()\n",
        "        # self.test_residual_analysis()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def predict_with_uncertainty(self, n_samples=100):\n",
        "        \"\"\"\n",
        "        Make predictions with uncertainty estimation using Monte Carlo Dropout\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_samples : int\n",
        "            Number of Monte Carlo samples\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        mean_predictions : numpy.ndarray\n",
        "            Mean predicted values\n",
        "\n",
        "        std_predictions : numpy.ndarray\n",
        "            Standard deviation of predictions\n",
        "        \"\"\"\n",
        "\n",
        "        if self.model is None:\n",
        "            print(\"Model not available. Run build_model and train_model first.\")\n",
        "            return None, None\n",
        "\n",
        "        if self.data is None:\n",
        "            print(\"Data not available. Run fetch_and_preprocess_data first.\")\n",
        "            return None, None\n",
        "\n",
        "        print(f\"Making predictions with uncertainty for {self.data['X_test'].shape[0]} samples\")\n",
        "\n",
        "        X_test_tensor = torch.tensor(self.data['X_test'], dtype=torch.float32).to(self.device)\n",
        "\n",
        "        self.model.train()  # Set to train mode to enable dropout\n",
        "\n",
        "        # Monte Carlo sampling\n",
        "        predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(n_samples):\n",
        "                outputs, _ = self.model(X_test_tensor, apply_dropout=True)\n",
        "                predictions.append(outputs.cpu().numpy())\n",
        "\n",
        "                if (i+1) % 10 == 0:\n",
        "                    print(f\"Completed {i+1}/{n_samples} Monte Carlo samples\")\n",
        "\n",
        "        # Calculate mean and standard deviation\n",
        "        predictions = np.array(predictions)\n",
        "        mean_predictions = np.mean(predictions, axis=0)\n",
        "        std_predictions = np.std(predictions, axis=0)\n",
        "\n",
        "        # Save predictions\n",
        "        np.save(os.path.join(f'{self.model_save_path}/results', 'mean_predictions.npy'), mean_predictions)\n",
        "        np.save(os.path.join(f'{self.model_save_path}/results', 'std_predictions.npy'), std_predictions)\n",
        "\n",
        "        return mean_predictions, std_predictions\n",
        "\n",
        "    def visualize_predictions(self, mean_predictions=None, std_predictions=None):\n",
        "        \"\"\"\n",
        "        Visualize model predictions\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        mean_predictions : numpy.ndarray\n",
        "            Mean predicted values from Monte Carlo Dropout\n",
        "\n",
        "        std_predictions : numpy.ndarray\n",
        "            Standard deviation of predictions from Monte Carlo Dropout\n",
        "        \"\"\"\n",
        "\n",
        "        if self.data is None:\n",
        "            print(\"Data not available. Please run fetch_and_preprocess_data first.\")\n",
        "            return\n",
        "\n",
        "        if self.predictions is None and mean_predictions is None:\n",
        "            print(\"No predictions available. Run evaluate_model or predict_with_uncertainty first.\")\n",
        "            return\n",
        "\n",
        "        print(\"Visualizing predictions\")\n",
        "\n",
        "        test_dates = [datetime.strptime(d, '%Y-%m-%d') for d in self.data['metadata']['test_dates']]\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Plot actual values\n",
        "        plt.plot(test_dates, self.data['y_test'], label='Actual', color='blue')\n",
        "\n",
        "        if mean_predictions is not None and std_predictions is not None:\n",
        "\n",
        "            plt.plot(test_dates, mean_predictions, label='Predicted', color='red', linestyle='--')\n",
        "            plt.fill_between(\n",
        "                test_dates,\n",
        "                mean_predictions.flatten() - 2 * std_predictions.flatten(),\n",
        "                mean_predictions.flatten() + 2 * std_predictions.flatten(),\n",
        "                color='red',\n",
        "                alpha=0.2,\n",
        "                label='95% Confidence Interval'\n",
        "            )\n",
        "        else:\n",
        "            plt.plot(test_dates, self.predictions, label='Predicted', color='red', linestyle='--')\n",
        "\n",
        "        plt.title(f'{self.ticker_name} - Forex Price Prediction')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Normalized Price')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        if len(test_dates) > 20:\n",
        "            plt.xticks(test_dates[::len(test_dates)//10])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(f'{self.model_save_path}/plots', 'predictions.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Plot prediction error\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        if mean_predictions is not None:\n",
        "            prediction_error = mean_predictions.flatten() - self.data['y_test']\n",
        "        else:\n",
        "            prediction_error = self.predictions - self.data['y_test']\n",
        "\n",
        "        plt.plot(test_dates, prediction_error, color='green')\n",
        "        plt.axhline(y=0, color='r', linestyle='-')\n",
        "        plt.title('Prediction Error')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Error (Predicted - Actual)')\n",
        "        plt.grid(True)\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # If there are too many dates, show only a subset\n",
        "        if len(test_dates) > 20:\n",
        "            plt.xticks(test_dates[::len(test_dates)//10])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(f'{self.model_save_path}/plots', 'prediction_error.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def _residual_analysis(self):\n",
        "\n",
        "        if self.model is None:\n",
        "            print(\"Model not available. Please run build_model and train_model first.\")\n",
        "            return None\n",
        "\n",
        "        if self.data is None:\n",
        "            print(\"Data not available. Please run fetch_and_preprocess_data first.\")\n",
        "            return None\n",
        "\n",
        "        if self.predictions is None:\n",
        "            print(\"No predictions available. Please run evaluate_model or predict_with_uncertainty first.\")\n",
        "            return None\n",
        "\n",
        "        residuals = self.data['y_test'] - self.predictions\n",
        "        mean_residuals = np.mean(residuals)\n",
        "        sd_residuals = np.std(residuals)\n",
        "        rmse = np.sqrt(np.mean(residuals**2))\n",
        "        index = [datetime.strptime(d, '%Y-%m-%d') for d in self.data['metadata']['test_dates']]\n",
        "\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(15,8))\n",
        "\n",
        "        sns.histplot(residuals, bins=50, ax=ax[0])\n",
        "        ax[0].axvline(mean_residuals + sd_residuals, color='grey', linestyle='--', linewidth=2)\n",
        "        ax[0].axvline(mean_residuals - sd_residuals, color='grey', linestyle='--', linewidth=2)\n",
        "        ax[0].axvline(mean_residuals, color='black', linewidth=3)\n",
        "        ax[0].text(x=mean_residuals, y=5, s= f'Mean: {mean_residuals: .2f}')\n",
        "        ax[0].text(x=mean_residuals + sd_residuals, y=3, s= f'Std: {sd_residuals: .2f}')\n",
        "        ax[0].set_title('Residual Distribution')\n",
        "        ax[0].set_xlabel('Residuals')\n",
        "        ax[0].set_ylabel('Counts')\n",
        "\n",
        "        qq = stats.probplot(residuals, dist=\"norm\", plot=None)\n",
        "        ax[1].scatter(qq[0][0], qq[1][1] + qq[1][0]*qq[0][0], label='Fitted Line')\n",
        "        ax[1].scatter(qq[0][0], qq[0][1], label='Predicted')\n",
        "        ax[1].set_title('QQ plots')\n",
        "        ax[1].set_xlabel('Theoretical quantiles')\n",
        "        ax[1].set_ylabel('Ordered values')\n",
        "\n",
        "        fig.suptitle(\"Residual Analysis\")\n",
        "\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(os.path.join(f'{self.model_save_path}/plots', 'residual_analysis.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"\n",
        "        Generate a comprehensive report of the model performance\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        report : dict\n",
        "            Dictionary containing the report\n",
        "        \"\"\"\n",
        "        if self.data is None or self.model is None or self.metrics is None:\n",
        "            print(\"Data, model, or metrics not available. Complete the training and evaluation first.\")\n",
        "            return None\n",
        "\n",
        "        print(\"Generating comprehensive report\")\n",
        "\n",
        "        # Create report\n",
        "        report = {\n",
        "            'model_summary': {\n",
        "                'ticker' : self.ticker_name,\n",
        "                'ticker_symbol' : self.ticker,\n",
        "                'input_shape': (self.data['X_train'].shape[1], self.data['X_train'].shape[2]),\n",
        "                'hyperparameters': self.params,\n",
        "                'training_samples': self.data['X_train'].shape[0],\n",
        "                'test_samples': self.data['X_test'].shape[0]\n",
        "            },\n",
        "            'evaluation_metrics': self.metrics,\n",
        "            'training_period': {\n",
        "                'start_date': self.start_date,\n",
        "                'end_date': self.end_date\n",
        "            },\n",
        "            'features_used': self.data['metadata']['features']\n",
        "        }\n",
        "\n",
        "        # Save report to file\n",
        "        with open(os.path.join(f'{self.model_save_path}/results', 'model_report.json'), 'w') as f:\n",
        "            json.dump(report, f, indent=4)\n",
        "\n",
        "        print(f\"Report generated and saved to {self.model_save_path}/results/model_report.json\")\n",
        "        return report"
      ],
      "metadata": {
        "id": "56ThcLB87uN7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to train and evaluate the prediction model\n",
        "    \"\"\"\n",
        "    print(\"Starting prediction model training and evaluation with PyTorch\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "\n",
        "    # Create directories\n",
        "    model_save_path =f'./model/{timestamp}'\n",
        "    if os.path.exists(model_save_path):\n",
        "      shutil.rmtree(model_save_path)\n",
        "\n",
        "    os.makedirs(f'{model_save_path}', exist_ok=True)\n",
        "    os.makedirs(f'{model_save_path}/data', exist_ok=True)\n",
        "    os.makedirs(f'{model_save_path}/models', exist_ok=True)\n",
        "    os.makedirs(f'{model_save_path}/results', exist_ok=True)\n",
        "    os.makedirs(f'{model_save_path}/plots', exist_ok=True)\n",
        "\n",
        "    # Optimized hyperparameters\n",
        "    optimized_params = {\n",
        "        'dropout_rate': 0.2,\n",
        "        'learning_rate': 0.0005,\n",
        "        'batch_size': 32,\n",
        "        'hidden_size_1': 128,\n",
        "        'hidden_size_2': 64,\n",
        "        'hidden_size_3': 32\n",
        "    }\n",
        "\n",
        "    # name and ticker\n",
        "    ticker_info = {'EURUSD': 'EURUSD=X'}\n",
        "\n",
        "    # date range\n",
        "    start_date = '2010-01-01'\n",
        "    end_date = None\n",
        "\n",
        "    # Fetch and preprocess data\n",
        "    data_handler = FinancialDataHandler(\n",
        "        ticker_info=ticker_info,\n",
        "        start_date=start_date,\n",
        "        end_date=end_date,\n",
        "        window_size=30,\n",
        "        train_split=0.8,\n",
        "        model_save_path=model_save_path\n",
        "    )\n",
        "\n",
        "    processed_data = data_handler.fetch_and_preprocess_data()\n",
        "\n",
        "    # Initialize model with optimized parameters\n",
        "    model = PredictionModel(\n",
        "        ticker_info,\n",
        "        processed_data,\n",
        "        start_date,\n",
        "        end_date,\n",
        "        device,\n",
        "        model_save_path,\n",
        "        optimized_params=optimized_params\n",
        "    )\n",
        "\n",
        "    # Build model\n",
        "    model.build_model()\n",
        "\n",
        "    # Train model\n",
        "    model.train_model(epochs=100, patience=20)\n",
        "\n",
        "    # Evaluate model\n",
        "    model.evaluate_model()\n",
        "\n",
        "    # Make predictions with uncertainty\n",
        "    mean_predictions, std_predictions = model.predict_with_uncertainty(n_samples=100)\n",
        "\n",
        "    # Visualize predictions\n",
        "    model.visualize_predictions(mean_predictions, std_predictions)\n",
        "\n",
        "    # Generate report\n",
        "    model.generate_report()\n",
        "\n",
        "    print(f\"{list(ticker_info.keys())[0]} prediction model training and evaluation completed\")\n",
        "    print(f\"Results and visualizations saved to {model_save_path}/ directory\")"
      ],
      "metadata": {
        "id": "2TkybLXfDxMC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCVEjs0iKFr8",
        "outputId": "36e04ba6-09a3-49e1-d2d5-0edfe80325ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting prediction model training and evaluation with PyTorch\n",
            "Fetching EURUSD data from 2010-01-01 to 2025-03-31\n",
            "Raw data saved to ./model/2025-03-31_00-08-13/data/EURUSD=X_raw.csv\n",
            "Calculating technical indicators\n",
            "Technical indicators calculated\n",
            "Preprocessing data\n",
            "Preprocessed data saved. X_train shape: (3136, 30, 20), y_train shape: (3136,)\n",
            "X_test shape: (785, 30, 20), y_test shape: (785,)\n",
            "Initialized Model with parameters: {'dropout_rate': 0.2, 'learning_rate': 0.0005, 'batch_size': 32, 'hidden_size_1': 128, 'hidden_size_2': 64, 'hidden_size_3': 32}\n",
            "Building stacked LSTM model with input size 20\n",
            "StackedLSTMModel(\n",
            "  (lstm1): LSTM(20, 128, batch_first=True)\n",
            "  (dropout1): Dropout(p=0.2, inplace=False)\n",
            "  (lstm2): LSTM(128, 64, batch_first=True)\n",
            "  (dropout2): Dropout(p=0.2, inplace=False)\n",
            "  (attention): AttentionLayer(\n",
            "    (attention): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            "  (lstm3): LSTM(128, 32, batch_first=True)\n",
            "  (dropout3): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 147298\n",
            "Training model with 3136 samples\n",
            "Epoch 1/100 - Train Loss: 0.027348, Val Loss: 0.042114\n",
            "Model saved at epoch 1 with validation loss: 0.042114\n",
            "Epoch 2/100 - Train Loss: 0.004221, Val Loss: 0.036899\n",
            "Model saved at epoch 2 with validation loss: 0.036899\n",
            "Epoch 3/100 - Train Loss: 0.003180, Val Loss: 0.039643\n",
            "Epoch 4/100 - Train Loss: 0.003295, Val Loss: 0.039835\n",
            "Epoch 5/100 - Train Loss: 0.003079, Val Loss: 0.048778\n",
            "Epoch 6/100 - Train Loss: 0.002926, Val Loss: 0.041402\n",
            "Epoch 7/100 - Train Loss: 0.002626, Val Loss: 0.045986\n",
            "Epoch 8/100 - Train Loss: 0.002498, Val Loss: 0.039669\n",
            "Epoch 9/100 - Train Loss: 0.002392, Val Loss: 0.032103\n",
            "Model saved at epoch 9 with validation loss: 0.032103\n",
            "Epoch 10/100 - Train Loss: 0.002526, Val Loss: 0.043469\n",
            "Epoch 11/100 - Train Loss: 0.002476, Val Loss: 0.039904\n",
            "Epoch 12/100 - Train Loss: 0.002575, Val Loss: 0.044730\n",
            "Epoch 13/100 - Train Loss: 0.002428, Val Loss: 0.035185\n",
            "Epoch 14/100 - Train Loss: 0.002118, Val Loss: 0.034923\n",
            "Epoch 15/100 - Train Loss: 0.002143, Val Loss: 0.035559\n",
            "Epoch 16/100 - Train Loss: 0.002260, Val Loss: 0.031066\n",
            "Model saved at epoch 16 with validation loss: 0.031066\n",
            "Epoch 17/100 - Train Loss: 0.002025, Val Loss: 0.039065\n",
            "Epoch 18/100 - Train Loss: 0.001999, Val Loss: 0.034647\n",
            "Epoch 19/100 - Train Loss: 0.001913, Val Loss: 0.036863\n",
            "Epoch 20/100 - Train Loss: 0.002000, Val Loss: 0.033264\n",
            "Epoch 21/100 - Train Loss: 0.002033, Val Loss: 0.031426\n",
            "Epoch 22/100 - Train Loss: 0.001942, Val Loss: 0.030418\n",
            "Model saved at epoch 22 with validation loss: 0.030418\n",
            "Epoch 23/100 - Train Loss: 0.001691, Val Loss: 0.030334\n",
            "Model saved at epoch 23 with validation loss: 0.030334\n",
            "Epoch 24/100 - Train Loss: 0.001759, Val Loss: 0.026738\n",
            "Model saved at epoch 24 with validation loss: 0.026738\n",
            "Epoch 25/100 - Train Loss: 0.001737, Val Loss: 0.028724\n",
            "Epoch 26/100 - Train Loss: 0.001603, Val Loss: 0.027840\n",
            "Epoch 27/100 - Train Loss: 0.001551, Val Loss: 0.024636\n",
            "Model saved at epoch 27 with validation loss: 0.024636\n",
            "Epoch 28/100 - Train Loss: 0.001577, Val Loss: 0.028160\n",
            "Epoch 29/100 - Train Loss: 0.001584, Val Loss: 0.028390\n",
            "Epoch 30/100 - Train Loss: 0.001463, Val Loss: 0.025207\n",
            "Epoch 31/100 - Train Loss: 0.001515, Val Loss: 0.024344\n",
            "Model saved at epoch 31 with validation loss: 0.024344\n",
            "Epoch 32/100 - Train Loss: 0.001411, Val Loss: 0.025352\n",
            "Epoch 33/100 - Train Loss: 0.001410, Val Loss: 0.022963\n",
            "Model saved at epoch 33 with validation loss: 0.022963\n",
            "Epoch 34/100 - Train Loss: 0.001386, Val Loss: 0.024204\n",
            "Epoch 35/100 - Train Loss: 0.001345, Val Loss: 0.022989\n",
            "Epoch 36/100 - Train Loss: 0.001315, Val Loss: 0.022204\n",
            "Model saved at epoch 36 with validation loss: 0.022204\n",
            "Epoch 37/100 - Train Loss: 0.001231, Val Loss: 0.022197\n",
            "Model saved at epoch 37 with validation loss: 0.022197\n",
            "Epoch 38/100 - Train Loss: 0.001276, Val Loss: 0.020323\n",
            "Model saved at epoch 38 with validation loss: 0.020323\n",
            "Epoch 39/100 - Train Loss: 0.001277, Val Loss: 0.022300\n",
            "Epoch 40/100 - Train Loss: 0.001352, Val Loss: 0.018831\n",
            "Model saved at epoch 40 with validation loss: 0.018831\n",
            "Epoch 41/100 - Train Loss: 0.001196, Val Loss: 0.020190\n",
            "Epoch 42/100 - Train Loss: 0.001153, Val Loss: 0.020506\n",
            "Epoch 43/100 - Train Loss: 0.001075, Val Loss: 0.018934\n",
            "Epoch 44/100 - Train Loss: 0.001059, Val Loss: 0.018232\n",
            "Model saved at epoch 44 with validation loss: 0.018232\n",
            "Epoch 45/100 - Train Loss: 0.001128, Val Loss: 0.019326\n",
            "Epoch 46/100 - Train Loss: 0.000972, Val Loss: 0.019333\n",
            "Epoch 47/100 - Train Loss: 0.000959, Val Loss: 0.017810\n",
            "Model saved at epoch 47 with validation loss: 0.017810\n",
            "Epoch 48/100 - Train Loss: 0.001004, Val Loss: 0.016571\n",
            "Model saved at epoch 48 with validation loss: 0.016571\n",
            "Epoch 49/100 - Train Loss: 0.000928, Val Loss: 0.015086\n",
            "Model saved at epoch 49 with validation loss: 0.015086\n",
            "Epoch 50/100 - Train Loss: 0.000870, Val Loss: 0.017662\n",
            "Epoch 51/100 - Train Loss: 0.000898, Val Loss: 0.016424\n",
            "Epoch 52/100 - Train Loss: 0.000975, Val Loss: 0.015396\n",
            "Epoch 53/100 - Train Loss: 0.000934, Val Loss: 0.015219\n",
            "Epoch 54/100 - Train Loss: 0.000930, Val Loss: 0.015361\n",
            "Epoch 55/100 - Train Loss: 0.000861, Val Loss: 0.017387\n",
            "Epoch 56/100 - Train Loss: 0.000805, Val Loss: 0.015612\n",
            "Epoch 57/100 - Train Loss: 0.000914, Val Loss: 0.014906\n",
            "Model saved at epoch 57 with validation loss: 0.014906\n",
            "Epoch 58/100 - Train Loss: 0.000850, Val Loss: 0.012407\n",
            "Model saved at epoch 58 with validation loss: 0.012407\n",
            "Epoch 59/100 - Train Loss: 0.000815, Val Loss: 0.014628\n",
            "Epoch 60/100 - Train Loss: 0.000850, Val Loss: 0.013295\n",
            "Epoch 61/100 - Train Loss: 0.000764, Val Loss: 0.012078\n",
            "Model saved at epoch 61 with validation loss: 0.012078\n",
            "Epoch 62/100 - Train Loss: 0.000786, Val Loss: 0.012370\n",
            "Epoch 63/100 - Train Loss: 0.000848, Val Loss: 0.013073\n",
            "Epoch 64/100 - Train Loss: 0.000779, Val Loss: 0.012462\n",
            "Epoch 65/100 - Train Loss: 0.000744, Val Loss: 0.011991\n",
            "Model saved at epoch 65 with validation loss: 0.011991\n",
            "Epoch 66/100 - Train Loss: 0.000763, Val Loss: 0.010608\n",
            "Model saved at epoch 66 with validation loss: 0.010608\n",
            "Epoch 67/100 - Train Loss: 0.000720, Val Loss: 0.014136\n",
            "Epoch 68/100 - Train Loss: 0.000759, Val Loss: 0.012100\n",
            "Epoch 69/100 - Train Loss: 0.000683, Val Loss: 0.010874\n",
            "Epoch 70/100 - Train Loss: 0.000694, Val Loss: 0.011492\n",
            "Epoch 71/100 - Train Loss: 0.000682, Val Loss: 0.011857\n",
            "Epoch 72/100 - Train Loss: 0.000713, Val Loss: 0.010590\n",
            "Model saved at epoch 72 with validation loss: 0.010590\n",
            "Epoch 73/100 - Train Loss: 0.000695, Val Loss: 0.011332\n",
            "Epoch 74/100 - Train Loss: 0.000686, Val Loss: 0.010788\n",
            "Epoch 75/100 - Train Loss: 0.000738, Val Loss: 0.011724\n",
            "Epoch 76/100 - Train Loss: 0.000762, Val Loss: 0.010408\n",
            "Model saved at epoch 76 with validation loss: 0.010408\n",
            "Epoch 77/100 - Train Loss: 0.000700, Val Loss: 0.011227\n",
            "Epoch 78/100 - Train Loss: 0.000572, Val Loss: 0.010929\n",
            "Epoch 79/100 - Train Loss: 0.000766, Val Loss: 0.011967\n",
            "Epoch 80/100 - Train Loss: 0.000674, Val Loss: 0.011033\n",
            "Epoch 81/100 - Train Loss: 0.000603, Val Loss: 0.009329\n",
            "Model saved at epoch 81 with validation loss: 0.009329\n",
            "Epoch 82/100 - Train Loss: 0.000649, Val Loss: 0.010045\n",
            "Epoch 83/100 - Train Loss: 0.000638, Val Loss: 0.010112\n",
            "Epoch 84/100 - Train Loss: 0.000611, Val Loss: 0.010721\n",
            "Epoch 85/100 - Train Loss: 0.000604, Val Loss: 0.010170\n",
            "Epoch 86/100 - Train Loss: 0.000649, Val Loss: 0.010539\n",
            "Epoch 87/100 - Train Loss: 0.000636, Val Loss: 0.011153\n",
            "Epoch 88/100 - Train Loss: 0.000582, Val Loss: 0.010634\n",
            "Epoch 89/100 - Train Loss: 0.000642, Val Loss: 0.010333\n",
            "Epoch 90/100 - Train Loss: 0.000602, Val Loss: 0.010565\n",
            "Epoch 91/100 - Train Loss: 0.000569, Val Loss: 0.009656\n",
            "Epoch 92/100 - Train Loss: 0.000623, Val Loss: 0.010164\n",
            "Epoch 93/100 - Train Loss: 0.000606, Val Loss: 0.010237\n",
            "Epoch 94/100 - Train Loss: 0.000625, Val Loss: 0.009473\n",
            "Epoch 95/100 - Train Loss: 0.000585, Val Loss: 0.009663\n",
            "Epoch 96/100 - Train Loss: 0.000581, Val Loss: 0.010309\n",
            "Epoch 97/100 - Train Loss: 0.000590, Val Loss: 0.010556\n",
            "Epoch 98/100 - Train Loss: 0.000585, Val Loss: 0.009372\n",
            "Epoch 99/100 - Train Loss: 0.000542, Val Loss: 0.009470\n",
            "Epoch 100/100 - Train Loss: 0.000602, Val Loss: 0.008950\n",
            "Model saved at epoch 100 with validation loss: 0.008950\n",
            "Evaluating model on 785 test samples\n",
            "Evaluation metrics: {'mse': 0.012097063626216096, 'mae': 0.10605591618141551, 'rmse': 0.10998665203658167, 'direction_accuracy': 0.5178571428571429, 'r2': -2.256746292114258}\n",
            "Making predictions with uncertainty for 785 samples\n",
            "Completed 10/100 Monte Carlo samples\n",
            "Completed 20/100 Monte Carlo samples\n",
            "Completed 30/100 Monte Carlo samples\n",
            "Completed 40/100 Monte Carlo samples\n",
            "Completed 50/100 Monte Carlo samples\n",
            "Completed 60/100 Monte Carlo samples\n",
            "Completed 70/100 Monte Carlo samples\n",
            "Completed 80/100 Monte Carlo samples\n",
            "Completed 90/100 Monte Carlo samples\n",
            "Completed 100/100 Monte Carlo samples\n",
            "Visualizing predictions\n",
            "Generating comprehensive report\n",
            "Report generated and saved to ./model/2025-03-31_00-08-13/results/model_report.json\n",
            "EURUSD prediction model training and evaluation completed\n",
            "Results and visualizations saved to ./model/2025-03-31_00-08-13/ directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNe1rKf-Ac9F"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}